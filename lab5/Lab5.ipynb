{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83340137",
   "metadata": {},
   "source": [
    "Na tych zajęciach poćwiczymy uczenie ze wzmocnieniem na prostym przykładzie balansowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()  \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1d868",
   "metadata": {},
   "source": [
    "Jak widać wykonywanie losowych czynności nie jest najlepszym podejściem. Mimo, że w tym przypadku pewnie jesteśmy w stanie napisać ręcznie algorytm rozwiązujący to zadanie to ciężko być ekspertem w każdej dziedzinie będącym w stanie stworzyć odpowiednią procedurę, a nawet gdyby to i tak nie będziemy w stanie uwzględnić wszystkich możliwych sytuacji. Z tego powodu skorzystamy z uczenia ze wzmocnieniem. Niczym podczas nauki psa sztuczek przy użyciu smakołyków będziemy uczyć agenta odpowiednich reakcji nagradzając te prawidłowe i karząc błędne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3480f",
   "metadata": {},
   "source": [
    "Zapoznajmy się najpierw z opisem problemu https://www.gymlibrary.dev/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0f50f",
   "metadata": {},
   "source": [
    "Zgodnie z opisem mamy 2 możliwe ruchy. Zobaczmy jak wygląda wyjście"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(80):\n",
    "    env.step(0)        \n",
    "    env.step(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674bd9a8",
   "metadata": {},
   "source": [
    "Użyjmy sieci neuronowej do oceniania które ruchy są dobre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e0bfe",
   "metadata": {},
   "source": [
    "#### Zad\n",
    "Stwórz model sieci neuronowej, który na wejściu otrzymuje obserwację ze środowiska a na wyjściu będzie zwracał ocenę każdego z możliwych ruchów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9519d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0385c846",
   "metadata": {},
   "source": [
    "#### Zad\n",
    "Najprostszym podejściem byłoby nauczyć model przewidywania nagrody po danym ruchu, spróbujmy czegoś takiego. Wykonuj losowe ruchy obserwuj uzyskaną nagrodę i w ten sposób przygotuj dane do treningu modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "for _ in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for _ in range(400):\n",
    "        action = env.action_space.sample() \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbc140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784057c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f1b3b4",
   "metadata": {},
   "source": [
    "#### Zad\n",
    "Przygotuj funkcję oceniającą uzyskane wyniki -- dla wybranego zestawu testowego (seedy przy tworzeniu środowiska) policz jak długo agent jest w stanie uniknąć przegranej. Możesz także przygotować mniejszy zestaw testowy do wizualizacji - obejrzenia jak agent sobie radzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAgent(model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73ba9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848dcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca16b55",
   "metadata": {},
   "source": [
    "#### Zad\n",
    "Można trochę poprawić to podejście. Mamy tutaj bardzo prostą nagrodę w binarnej formie. Czy jesteś w stanie w prosty sposób zmodyfikować nagrodę tak, żeby była bardziej ciągła? Czy uzyskane wyniki są lepsze?\n",
    "\n",
    "<details>\n",
    "<summary>Kliknij po podpowiedź jeśli nic nie przychodzi Ci do głowy</summary>\n",
    "Interesuje nas czy pręt jest odchylony oraz czy zbliżamy się do krawędzi planszy\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "for _ in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for _ in range(400):\n",
    "        action = env.action_space.sample() \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        reward = reward + 0\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1a90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c105904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda0ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7e4f7a",
   "metadata": {},
   "source": [
    "#### Zad\n",
    "Warto skorzystać z podejścia Q-learning i równania Bellmana, zapoznaj się z tymi podejściami i zmodyfikuj kod tak aby wykorzystać Deep Q-learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f5d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c61c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1969f5c0",
   "metadata": {},
   "source": [
    "### Zad\n",
    "Stwórz jak najlepszego agenta dla tego zadania lub dowolnego innego (ale nie prostszego) przetestuj różne podejścia i wybierz to, które spisuje się jak najlepiej. Ze względu na święta termin oddania projektu to 12 kwietnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f092234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
