{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93304745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, concatenate, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import Sequence, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4a88c",
   "metadata": {},
   "source": [
    "Podczas zajęć będziemy używać TensorFlow oraz Kerasa, ale projekt może być zrobiony w dowolnej technologii. Można nawet użyć czystego numpy, albo C czy Assemblera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2f032",
   "metadata": {},
   "source": [
    "TensorFlow z automatu rezerwuje sobie sporo pamięci na GPU, aby temu zapobiec należy ustawić parametr memory_growth, który pozwala mu dynamicznie zwiększać zajętość pamięci w zależności od potrzeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455fa82f",
   "metadata": {},
   "source": [
    "Zanim przejdziemy do sieci neuronowych zacznijmy od pojedynczego perceptronu. To prosty model matematyczny, który liczy sumę ważoną a następnie przykłada wybraną funkcję zwaną funkcją aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,))) \n",
    "# that's how we create a single layer. First parameter specifies how many neurons do we want, since we want to have\n",
    "# a single perceptron we use 1. Then we specify the input shape so the number of variables \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb9cc6",
   "metadata": {},
   "source": [
    "Powyżej stworzyliśmy prostą sieć neuronową z jednym neuronem, która przyjmuje wektor o długości dwa na wejściu. Jak widać taka sieć ma 3 parametry. Dwa z nich to wagi przypisane do każdego z wejść. Dodatkowy parametr, tak zwany bias, to stała\n",
    "\n",
    "Wzór na pojedynczy neuron można zdefiniować tak\n",
    "$$ \\sum (x_i*w_i) + bias$$\n",
    "\n",
    "Sprawdźmy czy to rzeczywiśie prawda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f5cc7",
   "metadata": {},
   "source": [
    "To są wagi naszego modelu - bias oraz wagi wejść. Są one generowane w sposób losowy, więc przy każdm wywołaniu mogą być zupełnie inne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[2,1]]) # dummy input for calculations\n",
    "print(model(x))\n",
    "print(x @ model.weights[0].numpy() + model.weights[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b73b40",
   "metadata": {},
   "source": [
    "Wygląda na to, że rzeczywiście dokładnie takie obliczenia mają miejsce, gdyż wynik wywołania modelu jest tożsamy z przeprowadzonymi obliczeniami. Znak @ odpowiada za mnożenie macierzy, jeśli nie ufasz możesz sprawdzić implementując swoje mnożenie macierzy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb4103",
   "metadata": {},
   "source": [
    "Ale miała być jeszcze funkcja aktywacji\n",
    "\n",
    "Zgadza się, tutaj używamy liniowej fukncji f(x) = x, stąd nie ma ona wpływu na wynik, ale można użyć praktycznie dowolnej funkcji, która jest później aplikowana do wyniku sumy ważonej, zaktualizujmy więc nasz wzór na perceptron\n",
    "$$ f(\\sum (x_i*w_i) + bias)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, 'tanh', input_shape=(2,))) # the second parameter specifies the activation function\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875894fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x), x @ model.weights[0].numpy() + model.weights[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77db638",
   "metadata": {},
   "source": [
    "Teraz wyniki się rozjechały. To dlatego, że model przykłada jeszcze funkcję tanh. Musimy ją dodać do naszych obliczeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tanh(x @ model.weights[0].numpy() + model.weights[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94068c",
   "metadata": {},
   "source": [
    "Wyniki mogą się odrobinę różnić, wynika to z dokładności numerycznej i różnej implementacji funkcji tanh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1868c9e",
   "metadata": {},
   "source": [
    "Jasne już jest jak działa neuron z ustalonymi wagami, ale skąd je wziąć? \n",
    "\n",
    "Pomysł jest trywialnie prosty. Potrzebujemy zbioru referencyjnego (treningowego) w którym mamy predyktory ($x$) oraz target ($y$). Następnie szukamy takich wag, które jak najwierniej mapują $x$ na $y$. W naszym przykładzie spróbujemy przewidzieć cenę mieszkania ($y$) na podstawie jego powierzchni ($x$). Oczywiście w tego typu zadaniu warto użyć więcej cech jak piętro, lokalizacja, rok budowy itp, ale dla prostoty użyjemy tylko jednej cechy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d07231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(41)\n",
    "x = np.random.rand(200,1)*20+50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22068e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(27)\n",
    "y = .5*x + np.random.rand(*x.shape)*3 + np.log(x-49)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, '.')\n",
    "plt.xlabel(\"$m^2$\")\n",
    "plt.ylabel(\"price in bitcoins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6689f",
   "metadata": {},
   "source": [
    "Stworzyliśmy zbiór danych, teraz potrzebujemy sieci neuronowej. Jaka powinna być funkcja aktywacji?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b59edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,))) #now we have only one input parameter\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0170d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2deab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, '.', label='data')\n",
    "plt.plot(np.sort(x,0), model.predict(np.sort(x,0)), label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e57d7",
   "metadata": {},
   "source": [
    "To nasza predykcja. Prawdopodobnie nie jesteśmy nawet blisko. Nie ma się co dziwić, w końcu mamy losowe wagi. Teraz musimy zdefiniować funkcję określającą na ile nasza predykcja odbiega od tej oczekiwanej - funkcja straty. Kiedy będziemy ją mieli możliwe będzie ocenienie danego wektora wag, więc problem znalezienia odpowiednich parametrów staje się zwykłym zadaniem optymalizacyjnym. Należy tak dobrać parametry, żeby zminimalizować funkcję straty. W praktyce korzysta się z faktu iż funkcja straty jest ciągła i różniczkowalna dzięki czemu można użyć algorytm spadku gradientu, ale nic nie stoi na przeszkodzie aby użyć algorytm genetyczny, czy nawet random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe24098",
   "metadata": {},
   "source": [
    "Pierwszym pomysłem na funkcję straty może być $prediction - y$. Oczywiście to podejście zawiedzie, gdyż pozytywne i negatywne błędy będą się znosić nawzajem. Jednym z rozwiązać może być policzenie modułu z tej miary. Jest to już poprawne podejście, jednak funkcja ta nie jest różniczkowalna, stąd w praktyce często stosuje się kwadrat tej miary $(prediction - y)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda x,y: ((y - x)**2).mean() # our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeca8e3",
   "metadata": {},
   "source": [
    "Sprawdźmy czy rzeczywiście można użyć random searcha do treningu sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd76d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestWeights = model.get_weights()\n",
    "pred = model.predict(x, verbose=0)\n",
    "bestError = mse(y, pred)\n",
    "bestError\n",
    "\n",
    "for _ in range(200):\n",
    "    weights = [np.random.randn(1,1), np.random.randn(1)]\n",
    "    model.set_weights(weights)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    err = mse(y, pred)\n",
    "    if err < bestError:\n",
    "        bestError = err\n",
    "        bestWeights = model.get_weights()\n",
    "        print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a93991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(bestWeights)\n",
    "plt.plot(x,y, '.', label='data')\n",
    "plt.plot(np.sort(x,0), model.predict(np.sort(x,0)), label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f789d7",
   "metadata": {},
   "source": [
    "Wygląda nieźle, no ale oczywiście nie jest to najlepsze podejście. Nie dość, że istnieją lepsze techniki, to są już zaimplementowane, więc nie trzeba się samemu wysilać"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43b5f6",
   "metadata": {},
   "source": [
    "Przy treninu specyfikuje się dwa podstawowe parametry rozmiar batcha oraz liczba epok. Liczba epok mówi nam o tym ile razy przeiterujemy się po całym zbiorze danych w fazie treningu, rozmiar batcha określa ile wierszy na raz będziemy rozważać podczas jednego kroku obliczania aktualizacji wag. Jak łatwo się domyślić liczba aktualizacji to liczba epok * rozmiar danych / batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c810b9",
   "metadata": {},
   "source": [
    "Zanim użyjemy gotowej funkcji treningu musimy skompilować model i zdefiniować funkcję straty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb372be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.compile(loss='mse', metrics='mae')\n",
    "model.fit(x,y, epochs=300, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6629144",
   "metadata": {},
   "source": [
    "Jak dobrać te parametry? W uproszczeniu batch size najczęściej chcemy mieć tak duży jak to możliwe - ogranicza nas tutaj pamięć, zazwyczaj karty graficznej. Liczba epok to bardziej złożony temat. Jak widać na naszym przykładzie w pewnym momencie dalszy trening nie ma już sensu, tylko jak to wykryć? Są gotowe zaimplementowace funkcje - callbacki. Jedna z nich EarlyStopping przerywa trening, jeśli funkcja straty nie poprawiła się przez założoną liczbę epok. Warto też rozważyć użycie ReduceLROnPlateau, który w analogicznej sytuacji zmniejsza stałą uczenia pozwalając na drobniejsze kroki - precyzyjniesze zbliżenie się do punktu optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early = EarlyStopping(monitor='loss', patience=15, restore_best_weights=True)\n",
    "reduce = ReduceLROnPlateau(monitor='loss', patience=6)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.compile(loss='mse', metrics='mae')\n",
    "model.fit(x,y, epochs=1000, batch_size=16, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, '.', label='data')\n",
    "plt.plot(np.sort(x,0), model.predict(np.sort(x,0)), label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ed606",
   "metadata": {},
   "source": [
    "Wynik nie jest idealny, ale w tym wypadku nie da się lepiej. Po prostu model liniowy nie jest w stanie wierniej zmapować $x$ na $y$. Nie istnieje prosta, która dużo lepiej dopasuje się do danych. Możemy np zwiększyć liczbę neuronów. Zazwyczaj zorganizowane są one w warstwy, wszystkie neurony z warstwy poprzedniej są połączone z wszystkim neuronami z warstwy następnej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380db2e4",
   "metadata": {},
   "source": [
    "Możemy też zmienić funkcję aktywacji. Najczęściej używane to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd2fa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"linear\")\n",
    "r = np.linspace(-7,7)\n",
    "plt.plot(r,r)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(\"tanh\")\n",
    "plt.plot(r, np.tanh(r))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(\"sigmoid\")\n",
    "plt.plot(r, 1/(1 + np.exp(-r)))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(\"relu\")\n",
    "plt.plot(r, np.where(r<0,0,r))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(\"leakyrelu\")\n",
    "plt.plot(r, np.where(r<0,r*.1,r))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029aa65",
   "metadata": {},
   "source": [
    "Jak zdecdować którą wybrać? Jak widać niektóre z nich mają ograniczony zakres wartości np sigmoid od 0 do 1, dlatego nadaje się on idealnie do sytuacji, w których wynik ma być traktowany jak prawdopodobieństwo. \n",
    "\n",
    "Przy kilku warstwach nie opłaca się stosować liniowej aktywacji - wiesz dlaczego?\n",
    "\n",
    "Dobry pierwszy wybór to zazwyczaj relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcc60a",
   "metadata": {},
   "source": [
    "Proces doboru wag używa pochodnych funkcji aktywacji. Jak widać dla większości z nich największa zmienność pochodnej jest w okolicy zera dlatego chcemy mieć wartości w tej okolicy. W tym celu dobrze jest odpowiednio przeskalować dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbc625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss_x = StandardScaler()\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "transformed_x = ss_x.fit_transform(x)\n",
    "transformed_y = ss_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022528f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(1,))) # 64 neurons in the first layer\n",
    "model.add(Dense(1)) # no need to specify input shape. Why?\n",
    "\n",
    "model.compile(loss='mse', metrics='mae')\n",
    "\n",
    "early = EarlyStopping(monitor='loss', patience=15, restore_best_weights=True)\n",
    "reduce = ReduceLROnPlateau(monitor='loss', patience=6)\n",
    "\n",
    "model.fit(transformed_x,transformed_y, epochs=500, batch_size=16, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a731df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(transformed_x,transformed_y, '.', label='data')\n",
    "plt.plot(np.sort(transformed_x, 0), model.predict(np.sort(transformed_x, 0)), label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e3ede",
   "metadata": {},
   "source": [
    "Funkcja sortująca jest tylko po to, żeby narysować ładną linię. Jeśli jej nie użyjemy to można narysować punkty, ale połączenie ich odcinkami da niezbyt dobry rezultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(transformed_x,transformed_y, '.', label='data')\n",
    "plt.plot(transformed_x, model.predict(transformed_x), label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(transformed_x,transformed_y, '.', label='data')\n",
    "plt.plot(transformed_x, model.predict(transformed_x), '.', label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c826308",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Potestuj różne architektury https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180a5f3",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Zobacz jaka architektura działa najlpiej dla naszego problemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967903a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(1,))) \n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', metrics='mae')\n",
    "\n",
    "early = EarlyStopping(monitor='loss', patience=15, restore_best_weights=True)\n",
    "reduce = ReduceLROnPlateau(monitor='loss', patience=6)\n",
    "\n",
    "model.fit(transformed_x,transformed_y, epochs=500, batch_size=16, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(1,))) \n",
    "model.add(Dense(128, activation='relu')) \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', metrics='mae')\n",
    "\n",
    "early = EarlyStopping(monitor='loss', patience=15, restore_best_weights=True)\n",
    "reduce = ReduceLROnPlateau(monitor='loss', patience=6)\n",
    "\n",
    "model.fit(transformed_x,transformed_y, epochs=500, batch_size=16, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdc31e",
   "metadata": {},
   "source": [
    "Wybranie liczby neuronów i warstw jest samo w sobie zagadnieniem optymalizacyjnym. Nie ma ścisłych reguł, generalnie należy zwiększać rozmiar siecii jeśli poprawia się jakość, ale jednocześnie uważać na przeuczenie. Zawsze używaj osobnego zbioru walidacyjnego i testowego.\n",
    "\n",
    "Pamiętaj, że nawet najbardziej złożona sieć nadal działa jak pojedynczy perceptron -- to po prostu równanie matematyczne z wagami wybranymi przez algorytm optymalizacji.\n",
    "\n",
    "Tip: skoro dobór architektóry to problem optymalizacyjny to możemy użyć algorytmu optymalizującego żeby go rozwiązać. Oczywistym pomysłem na reprezentację jest wektor z liczbą neuronów w poszczególnych warstwach. Niestety to podejście ma swoje minusy. W skrajnej sytuacji np w drugiej warstwie może być 0 neuronów a w trzeciej >0. W ogólności w tego typu sieciach należy raczej unikać zwiększania liczby neuronów w kolejnej warstwie. Ciekawym podejściem jest taka reprezentacja w której pierwsza liczba to liczba neuronów w pierwszej warstwie a kolejne liczby to procent neuronów z warstwy poprzedniej. Przykładowo $[64, 1, .5, .5, 0, 0]$ oznacza model z 4 warstwami w których jest odpowiednio 64, 64, 32, oraz 16 neuronów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int = np.int64\n",
    "def loss(x):\n",
    "    model = Sequential()\n",
    "    neurons = int(x[0])\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(train_x.shape[1],)))\n",
    "    for i in x[1:]:\n",
    "        neurons = int(i*neurons)\n",
    "        if neurons < 2:\n",
    "            break\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', metrics='mae')\n",
    "\n",
    "    early = EarlyStopping(patience=15, restore_best_weights=True)\n",
    "    reduce = ReduceLROnPlateau(patience=6)\n",
    "\n",
    "    model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=500, batch_size=16, callbacks=[early, reduce], verbose=0)\n",
    "    res = model.evaluate(val_x, val_y, verbose=0)\n",
    "    print(res)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "\n",
    "np.random.seed(31)\n",
    "idx = np.arange(len(x))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train = idx[:int(.8*len(x))]\n",
    "val = idx[int(.8*len(x)):int(.9*len(x))]\n",
    "test = idx[int(.9*len(x)):]\n",
    "\n",
    "train_x, train_y = transformed_x[train], transformed_y[train]\n",
    "val_x, val_y = transformed_x[val], transformed_y[val]\n",
    "test_x, test_y = transformed_x[test], transformed_y[test]\n",
    "\n",
    "result = gp_minimize(loss, [(16,256), (0,1.0), (0,1.0), (0,1.0)], n_calls=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a36d2",
   "metadata": {},
   "source": [
    "Aby uniknąć przeuczenia dzielimy zbiór danych na treningowy, walidacyjny i testowy. W większości przypadków warto wcześniej przemieszać dane aby uniknąć problemów związanych z nieprzypadkowym ułożeniem danych, przykładowo mogą być posortowane według targetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2164e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(31)\n",
    "idx = np.arange(len(x))\n",
    "np.random.shuffle(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50911996",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = idx[:int(.8*len(x))]\n",
    "val = idx[int(.8*len(x)):int(.9*len(x))]\n",
    "test = idx[int(.9*len(x)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = transformed_x[train], transformed_y[train]\n",
    "val_x, val_y = transformed_x[val], transformed_y[val]\n",
    "test_x, test_y = transformed_x[test], transformed_y[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432061e0",
   "metadata": {},
   "source": [
    "Problem jest tu taki, że skorzystaliśmy ze wszystkich danych, żeby wytrenować skaler danych, nie jest to duży błąd, ale należy unikać tego typu rozwiązań. \n",
    "\n",
    "#### Task\n",
    "Podziel oryginalne dane na poszczególne zbiory i odpowiednio je przeskaluj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b33ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(1,))) \n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', metrics='mae')\n",
    "\n",
    "early = EarlyStopping(patience=15, restore_best_weights=True) # we don't specify monitor, by default it's val_loss\n",
    "reduce = ReduceLROnPlateau(patience=6)\n",
    "\n",
    "model.fit(train_x,train_y, validation_data=(val_x, val_y), epochs=500, batch_size=16, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(test_x,test_y, '.', label='data')\n",
    "plt.plot(np.sort(test_x, 0), model.predict(np.sort(test_x, 0)), label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f8d7f",
   "metadata": {},
   "source": [
    "Dwa popularne modele na tworzenie sieci neuronowych to Sequential i Model. Pierwszy dobrze działa dla prostych sekwencyjnych sieci, za pomocą drugiego można tworzyć bardzie złożone modele.\n",
    "\n",
    "W sekwencyjnych modelach zakładamy, że warstwy są wywoływana po kolei jedna po drugiej. W Model, każdą warstwę traktujemy jaku funkcję i można je dowolnie zagnieżdżać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aeb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(1,))) \n",
    "model.add(Dense(32, activation='relu', input_shape=(1,))) \n",
    "model.add(Dense(16, activation='relu', input_shape=(1,))) \n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85206c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fcfaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = Input(shape=(1,))\n",
    "dense1 = Dense(64, activation='relu')(inputLayer)\n",
    "dense2 = Dense(32, activation='relu')(dense1)\n",
    "dense3 = Dense(16, activation='relu')(dense2)\n",
    "dense4 = Dense(1, activation='relu')(dense3)\n",
    "model = Model(inputs=inputLayer, outputs=dense4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1916b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = Input(shape=(1,))\n",
    "dense1 = Dense(64, activation='relu')(inputLayer)\n",
    "dense2 = Dense(32, activation='relu')(dense1)\n",
    "concat = Concatenate()([dense1, dense2])\n",
    "dense3 = Dense(16, activation='relu')(concat)\n",
    "dense4 = Dense(1, activation='relu')(dense3)\n",
    "model = Model(inputs=inputLayer, outputs=dense4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44142fc3",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Stwórz model z dwoma wejściami, pierwsze przetwarzane przez 3 warstwy Dense, drugie przez dwie. Następnie połącz je, przepuść przez 2 warswy Dense i rozdziel na dwa wyjścia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d4b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e28c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a293aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d851d7c",
   "metadata": {},
   "source": [
    "Generator danych to przydatne narzędzie do treningu sieci neuronowych. Pozwala ono na tworzenie każdego batcha danych niezależnie. Dzięki temu nie ma konieczności trzymania całego zbioru w pamięci co może być problematyczne np przy dużym zbiorze z obrazkami. Można skorzystać z tej funkcjonalności także przy augmentacji danych lub ich generowaniu w locie na podstawie zdefiniowanej funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x, y, batch_size, shuffle=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.indexes = np.arange(len(y))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch. \n",
    "        # During training and prediction this function will be called in range(0, __len__())\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, idx):\n",
    "        X = np.empty((self.batch_size, 1))\n",
    "        y = np.empty((self.batch_size), )\n",
    "\n",
    "        for i, ID in enumerate(idx):\n",
    "            # Store sample\n",
    "            X[i,] = self.x[ID]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.y[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d621e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGenerator = DataGenerator(train_x, train_y, 16)\n",
    "valGenerator = DataGenerator(val_x, val_y, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(1,))) \n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', metrics='mae')\n",
    "\n",
    "early = EarlyStopping(patience=15, restore_best_weights=True) # we don't specify monitor, by default it's val_loss\n",
    "reduce = ReduceLROnPlateau(patience=6)\n",
    "\n",
    "model.fit(trainGenerator, validation_data=valGenerator, epochs=500, batch_size=16, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e83f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_x,test_y, '.', label='data')\n",
    "plt.plot(np.sort(test_x, 0), model.predict(np.sort(test_x, 0)), label='prediction')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34890d07",
   "metadata": {},
   "source": [
    "Oczywiście w tym wypadku generator nie ma zbyt wiele sensu. Wszystkie dane i tak są w pamięci i nic z nimi nie robimy. Natomiast jest to baza, którą można wykorzystać gdy zajdzie taka potrzeba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45ea08",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Używając sieci neuronowej zapredykuj jakość wina:\n",
    " - traktując target jako zmienną ciągłą (regresja)\n",
    " - traktując target jako zmienną dyskretną (klasyfikacja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb56e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086aeb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a18920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a912776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd7d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54faf924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee0e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819228bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try also this model\n",
    "#check why and how it works\n",
    "model = Sequential()\n",
    "model.add(Dense(16, 'relu', input_shape=(13,)))\n",
    "model.add(Dense(3, 'softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=500, batch_size=16, callbacks=[early, reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31694efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a209a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f7d71bd",
   "metadata": {},
   "source": [
    "## Projekt\n",
    "Podczas pierwszych zajęć zadaniem było stworzenie w numpy funkcji, która wyliczy prawdopodobieństwo wygrania wyborów. Stwórz i wytrenuj sieć neuronową, która realizuje to zadanie. Oczywiście sieć będzie zwracać przybliżone wartości, za to znacznie szybciej. Załóż, że liczba potencjalnych kandydatów będzie nie większa niż 20. Jeśli model na sztywno przyjmuje na wejściu dane dla 20 potencjalnych kandydatów i jeśli będzie ich mniej to odpowiednie elemnty wejścia są wyzerowane to zadbaj o to, żeby na wyjściu dla nich również były zera\n",
    "\n",
    "Dodatkowe punkt za następujące elementy:\n",
    " - Wykorzystanie generatora danych\n",
    " - Dodanie warstwy lambda, która odpowiada za wstawienie zer w odpowiednie miejsca jeśli liczba kandydatów jest mniejsza niż 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87d9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
